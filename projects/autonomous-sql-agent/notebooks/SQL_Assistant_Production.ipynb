{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaad0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Erstelle die Ordnerstruktur\n",
    "import os\n",
    "os.makedirs(\"sql_project/scripts\", exist_ok=True)\n",
    "os.makedirs(\"sql_project/configs\", exist_ok=True)\n",
    "os.makedirs(\"sql_project/outputs\", exist_ok=True)\n",
    "\n",
    "# 2. Wechsel in das Projektverzeichnis\n",
    "%cd sql_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7209191",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "torch\n",
    "transformers\n",
    "peft\n",
    "bitsandbytes\n",
    "trl\n",
    "accelerate\n",
    "datasets\n",
    "huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876f6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c83b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/train.py\n",
    "import os\n",
    "import torch\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from huggingface_hub import login, HfApi\n",
    "\n",
    "def main(args):\n",
    "    # Hugging Face Hub Login\n",
    "    if args.push_to_hub:\n",
    "        print(\"Initialisiere Hugging Face Hub...\")\n",
    "        try:\n",
    "            # Try to login - if token is provided via environment or file, it will use that\n",
    "            # Otherwise, it will prompt for login\n",
    "            hf_token = os.environ.get(\"HF_TOKEN\") or args.hf_token\n",
    "            if hf_token:\n",
    "                login(token=hf_token)\n",
    "                print(\"\u2713 Erfolgreich bei Hugging Face angemeldet!\")\n",
    "            else:\n",
    "                print(\"Bitte melden Sie sich bei Hugging Face an...\")\n",
    "                login()  # This will prompt for token or use existing credentials\n",
    "                print(\"\u2713 Erfolgreich bei Hugging Face angemeldet!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Hugging Face Login Fehler: {e}\")\n",
    "            print(\"\u26a0\ufe0f Adapter wird nur lokal gespeichert.\")\n",
    "            args.push_to_hub = False\n",
    "    \n",
    "    # Wandb Setup - Automatically use existing account (option 2)\n",
    "    # Set environment to use existing credentials\n",
    "    os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "    # If wandb is not already logged in, this will use existing credentials from ~/.netrc or environment\n",
    "    try:\n",
    "        import wandb\n",
    "        # Check if wandb is already initialized - use a safer check\n",
    "        try:\n",
    "            current_run = wandb.run\n",
    "        except:\n",
    "            current_run = None\n",
    "        \n",
    "        if current_run is None:\n",
    "            wandb.init(project=\"sql-assistant\", mode=\"online\")\n",
    "    except Exception as e:\n",
    "        print(f\"Wandb initialization note: {e}\")\n",
    "        # Continue without wandb if there's an issue\n",
    "    # 1. Datensatz laden\n",
    "    print(f\"Lade Datensatz: {args.dataset_name}\")\n",
    "    dataset = load_dataset(args.dataset_name, split=\"train\")\n",
    "    # Nur f\u00fcr Demo-Zwecke verk\u00fcrzen, falls gew\u00fcnscht\n",
    "    if args.max_samples:\n",
    "        dataset = dataset.select(range(args.max_samples))\n",
    "    \n",
    "    # 2. Modell & Tokenizer laden (4-bit QLoRA)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(f\"Lade Modell: {args.model_name}\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.model_name, quantization_config=bnb_config, device_map=\"auto\"\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # 3. LoRA Config - f\u00fcr Qwen2.5 Modelle\n",
    "    peft_config = LoraConfig(\n",
    "        r=16, \n",
    "        lora_alpha=16, \n",
    "        lora_dropout=0.05, \n",
    "        bias=\"none\", \n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "    )\n",
    "    \n",
    "    # 4. Model f\u00fcr k-bit training vorbereiten und PEFT anwenden\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "\n",
    "    # 5. Training Arguments - Kombiniere TrainingArguments mit SFTConfig\n",
    "    training_args = SFTConfig(\n",
    "        output_dir=args.output_dir,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=args.lr,\n",
    "        logging_steps=10,\n",
    "        num_train_epochs=args.epochs,\n",
    "        fp16=False,  # Disable fp16 to avoid BFloat16 gradient scaler issue\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=1,\n",
    "        remove_unused_columns=False,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_length=512,\n",
    "        packing=False,\n",
    "        max_grad_norm=1.0  # Gradient clipping\n",
    "    )\n",
    "\n",
    "    # 6. Daten formatieren (Qwen Template)\n",
    "    def format_prompt(sample):\n",
    "        prompt = f\"<|im_start|>system\\nYou are a SQL expert.<|im_end|>\\n<|im_start|>user\\n{sample['context']}\\nQuestion: {sample['question']}<|im_end|>\\n<|im_start|>assistant\\n{sample['answer']}<|im_end|>\"\n",
    "        return {\"text\": prompt}\n",
    "\n",
    "    train_dataset = dataset.map(format_prompt, remove_columns=dataset.column_names)\n",
    "\n",
    "    # 7. Trainer Starten (ohne peft_config, da Modell bereits PEFT-wrapped ist)\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        processing_class=tokenizer,\n",
    "        args=training_args\n",
    "    )\n",
    "\n",
    "    print(\"Starte Training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Speichere Adapter nach {args.output_dir}...\")\n",
    "    trainer.model.save_pretrained(args.output_dir)\n",
    "    tokenizer.save_pretrained(args.output_dir)\n",
    "    \n",
    "    # Upload to Hugging Face Hub\n",
    "    if args.push_to_hub:\n",
    "        print(f\"\\n\ud83d\udce4 Lade Adapter auf Hugging Face hoch: {args.hf_repo_id}\")\n",
    "        try:\n",
    "            # Push adapter to Hub\n",
    "            trainer.model.push_to_hub(\n",
    "                args.hf_repo_id,\n",
    "                private=args.private_repo,\n",
    "                token=os.environ.get(\"HF_TOKEN\") or args.hf_token\n",
    "            )\n",
    "            print(f\"\u2713 Adapter erfolgreich hochgeladen: https://huggingface.co/{args.hf_repo_id}\")\n",
    "            \n",
    "            # Also push tokenizer\n",
    "            tokenizer.push_to_hub(\n",
    "                args.hf_repo_id,\n",
    "                private=args.private_repo,\n",
    "                token=os.environ.get(\"HF_TOKEN\") or args.hf_token\n",
    "            )\n",
    "            print(\"\u2713 Tokenizer erfolgreich hochgeladen!\")\n",
    "        except Exception as e:\n",
    "            print(f\"\u26a0\ufe0f Fehler beim Hochladen auf Hugging Face: {e}\")\n",
    "            print(\"\u26a0\ufe0f Adapter wurde lokal gespeichert.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "    parser.add_argument(\"--dataset_name\", type=str, default=\"b-mc2/sql-create-context\")\n",
    "    parser.add_argument(\"--output_dir\", type=str, default=\"./outputs/final_model\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=4)\n",
    "    parser.add_argument(\"--epochs\", type=int, default=1)\n",
    "    parser.add_argument(\"--lr\", type=float, default=2e-4)\n",
    "    parser.add_argument(\"--max_samples\", type=int, default=500) # Klein halten f\u00fcr Test\n",
    "    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Upload adapter to Hugging Face Hub\")\n",
    "    parser.add_argument(\"--hf_repo_id\", type=str, default=None, help=\"Hugging Face repo ID (e.g., 'username/model-name')\")\n",
    "    parser.add_argument(\"--hf_token\", type=str, default=None, help=\"Hugging Face token (or set HF_TOKEN env var)\")\n",
    "    parser.add_argument(\"--private_repo\", action=\"store_true\", help=\"Make the Hugging Face repo private\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Validate Hugging Face arguments\n",
    "    if args.push_to_hub and not args.hf_repo_id:\n",
    "        raise ValueError(\"--hf_repo_id is required when --push_to_hub is set\")\n",
    "    \n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f1a54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/evaluate.py\n",
    "import torch\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def normalize_sql(query):\n",
    "    \"\"\"Bereinigt SQL von Leerzeichen und Gro\u00df/Kleinschreibung f\u00fcr fairen Vergleich\"\"\"\n",
    "    if not query: return \"\"\n",
    "    query = query.lower().replace(\";\", \"\").replace(\"\\n\", \" \")\n",
    "    return \" \".join(query.split())\n",
    "\n",
    "def main(args):\n",
    "    # 1. Basis-Modell & Adapter laden\n",
    "    print(f\"Lade Basis-Modell: {args.base_model_name}\")\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.base_model_name, device_map=\"auto\", torch_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    print(f\"Lade Adapter: {args.adapter_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, args.adapter_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.base_model_name)\n",
    "\n",
    "    # 2. Test-Daten laden (Die letzten 100 Zeilen des Datasets als Testset nehmen)\n",
    "    dataset = load_dataset(\"b-mc2/sql-create-context\", split=\"train\")\n",
    "    test_dataset = dataset.select(range(len(dataset)-args.num_samples, len(dataset)))\n",
    "\n",
    "    correct_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    print(f\"Starte Evaluation auf {args.num_samples} Beispielen...\")\n",
    "\n",
    "    for sample in tqdm(test_dataset):\n",
    "        # Prompt bauen\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a SQL expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{sample['context']}\\nQuestion: {sample['question']}\"}\n",
    "        ]\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "        # Generieren\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "        \n",
    "        # Antwort extrahieren\n",
    "        generated_full = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        generated_sql = generated_full.split(\"assistant\")[-1].strip()\n",
    "        \n",
    "        # Vergleichen (Normalized Exact Match)\n",
    "        truth_norm = normalize_sql(sample[\"answer\"])\n",
    "        pred_norm = normalize_sql(generated_sql)\n",
    "\n",
    "        if truth_norm == pred_norm:\n",
    "            correct_count += 1\n",
    "        total_count += 1\n",
    "\n",
    "    accuracy = (correct_count / total_count) * 100\n",
    "    print(f\"\\n==========================================\")\n",
    "    print(f\"RESULTAT: Exact Match Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"==========================================\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--base_model_name\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "    parser.add_argument(\"--adapter_path\", type=str, required=True)\n",
    "    parser.add_argument(\"--num_samples\", type=int, default=50)\n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484bf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb Setup - Use existing account (option 2) non-interactively\n",
    "import os\n",
    "# Set wandb to use existing account without prompt\n",
    "os.environ[\"WANDB_MODE\"] = \"online\"\n",
    "# If you have a wandb API key, you can set it here to avoid prompts:\n",
    "os.environ[\"WANDB_API_KEY\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c37fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir trainieren das Modell und speichern es im Ordner 'outputs/v1'\n",
    "# Automatisch Option 2 (Use existing W&B account) ausw\u00e4hlen\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def run_with_auto_input(push_to_hub=False, hf_repo_id=None, hf_token=None, private_repo=False):\n",
    "    \"\"\"Run training script and automatically send '2' when wandb prompts\"\"\"\n",
    "    cmd = [sys.executable, \"scripts/train.py\",\n",
    "         \"--model_name\", \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "         \"--output_dir\", \"./outputs/v1\",\n",
    "         \"--epochs\", \"1\",\n",
    "         \"--max_samples\", \"500\"]\n",
    "    \n",
    "    # Add Hugging Face upload arguments if specified\n",
    "    if push_to_hub:\n",
    "        cmd.extend([\"--push_to_hub\"])\n",
    "        if hf_repo_id:\n",
    "            cmd.extend([\"--hf_repo_id\", hf_repo_id])\n",
    "        if hf_token:\n",
    "            cmd.extend([\"--hf_token\", hf_token])\n",
    "        if private_repo:\n",
    "            cmd.extend([\"--private_repo\"])\n",
    "    \n",
    "    process = subprocess.Popen(\n",
    "        cmd,\n",
    "        stdin=subprocess.PIPE,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "    \n",
    "    buffer = \"\"\n",
    "    sent_input = False\n",
    "    \n",
    "    while True:\n",
    "        # Read available output\n",
    "        output = process.stdout.read(1)\n",
    "        if not output:\n",
    "            if process.poll() is not None:\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "            continue\n",
    "            \n",
    "        buffer += output\n",
    "        print(output, end='', flush=True)\n",
    "        \n",
    "        # Check for wandb prompt and send \"2\" once\n",
    "        if \"Enter your choice:\" in buffer and not sent_input:\n",
    "            time.sleep(0.2)\n",
    "            process.stdin.write(\"2\\n\")\n",
    "            process.stdin.flush()\n",
    "            sent_input = True\n",
    "            buffer = \"\"\n",
    "        \n",
    "        # Keep buffer size manageable\n",
    "        if len(buffer) > 500:\n",
    "            buffer = buffer[-200:]\n",
    "    \n",
    "    return process.wait()\n",
    "\n",
    "# Training OHNE Hugging Face Upload (Standard)\n",
    "run_with_auto_input()\n",
    "\n",
    "# Training MIT Hugging Face Upload (auskommentiert - bitte aktivieren und anpassen)\n",
    "# run_with_auto_input(\n",
    "#     push_to_hub=True,\n",
    "#     hf_repo_id=\"your-username/your-model-name\",  # Z.B. \"manuelaschrittwieser99/qwen2.5-1.5b-sql-adapter\"\n",
    "#     hf_token=None,  # Optional: Token direkt \u00fcbergeben, oder setzen Sie HF_TOKEN als Umgebungsvariable\n",
    "#     private_repo=False  # True = privates Repo, False = \u00f6ffentliches Repo\n",
    "# )\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474200c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload eines bereits trainierten Adapters zu Hugging Face\n",
    "import torch\n",
    "from huggingface_hub import login, HfApi\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import os\n",
    "\n",
    "def upload_adapter_to_hub(\n",
    "    adapter_path=\"./outputs/v1\",\n",
    "    hf_repo_id=None,\n",
    "    base_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    hf_token=None,\n",
    "    private_repo=False\n",
    "):\n",
    "    \"\"\"\n",
    "    L\u00e4dt einen bereits trainierten Adapter und l\u00e4dt ihn auf Hugging Face hoch.\n",
    "    \n",
    "    Args:\n",
    "        adapter_path: Pfad zum lokal gespeicherten Adapter\n",
    "        hf_repo_id: Hugging Face Repo ID (z.B. \"username/model-name\")\n",
    "        base_model_name: Name des Basis-Modells\n",
    "        hf_token: Hugging Face Token (optional, kann auch als HF_TOKEN env var gesetzt werden)\n",
    "        private_repo: Ob das Repo privat sein soll\n",
    "    \"\"\"\n",
    "    \n",
    "    if not hf_repo_id:\n",
    "        raise ValueError(\"hf_repo_id ist erforderlich (z.B. 'username/model-name')\")\n",
    "    \n",
    "    # Login zu Hugging Face\n",
    "    print(\"Initialisiere Hugging Face Hub...\")\n",
    "    token = hf_token or os.environ.get(\"HF_TOKEN\")\n",
    "    if token:\n",
    "        login(token=token)\n",
    "        print(\"\u2713 Erfolgreich bei Hugging Face angemeldet!\")\n",
    "    else:\n",
    "        print(\"Bitte melden Sie sich bei Hugging Face an...\")\n",
    "        login()\n",
    "        print(\"\u2713 Erfolgreich bei Hugging Face angemeldet!\")\n",
    "    \n",
    "    # Lade Basis-Modell (nur zum Push ben\u00f6tigt, wird nicht geladen)\n",
    "    print(f\"\\n\ud83d\udce4 Lade Adapter von: {adapter_path}\")\n",
    "    \n",
    "    # F\u00fcr PEFT-Modelle m\u00fcssen wir das Basis-Modell laden, um den Adapter zu pushen\n",
    "    # Aber wir k\u00f6nnen es im 4-bit Modus laden, um Speicher zu sparen\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16\n",
    "        )\n",
    "        print(f\"Lade Basis-Modell: {base_model_name}\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name, \n",
    "            quantization_config=bnb_config, \n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Konnte Modell nicht im 4-bit Modus laden: {e}\")\n",
    "        print(\"Versuche ohne Quantisierung...\")\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_name,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "    \n",
    "    # Lade Adapter\n",
    "    print(f\"Lade Adapter von: {adapter_path}\")\n",
    "    model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "    \n",
    "    # Lade Tokenizer\n",
    "    print(\"Lade Tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "    \n",
    "    # Upload Adapter\n",
    "    print(f\"\\n\ud83d\udce4 Lade Adapter auf Hugging Face hoch: {hf_repo_id}\")\n",
    "    try:\n",
    "        model.push_to_hub(\n",
    "            hf_repo_id,\n",
    "            private=private_repo,\n",
    "            token=token\n",
    "        )\n",
    "        print(f\"\u2713 Adapter erfolgreich hochgeladen: https://huggingface.co/{hf_repo_id}\")\n",
    "        \n",
    "        # Upload Tokenizer\n",
    "        tokenizer.push_to_hub(\n",
    "            hf_repo_id,\n",
    "            private=private_repo,\n",
    "            token=token\n",
    "        )\n",
    "        print(\"\u2713 Tokenizer erfolgreich hochgeladen!\")\n",
    "        \n",
    "        print(f\"\\n\u2705 Fertig! Adapter ist verf\u00fcgbar unter: https://huggingface.co/{hf_repo_id}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\u26a0\ufe0f Fehler beim Hochladen: {e}\")\n",
    "        raise\n",
    "\n",
    "# BEISPIEL-VERWENDUNG:\n",
    "# Bitte passen Sie die Parameter an:\n",
    "\n",
    "# upload_adapter_to_hub(\n",
    "#     adapter_path=\"./outputs/v1\",  # Pfad zu Ihrem trainierten Adapter\n",
    "#     hf_repo_id=\"your-username/your-model-name\",  # Ihr Hugging Face Repo\n",
    "#     base_model_name=\"Qwen/Qwen2.5-1.5B-Instruct\",  # Basis-Modell\n",
    "#     hf_token=None,  # Optional: Token hier angeben, oder HF_TOKEN env var setzen\n",
    "#     private_repo=False  # True f\u00fcr privates Repo\n",
    "# )\n",
    "\n",
    "print(\"\u2713 Upload-Funktion bereit!\")\n",
    "print(\"\\nZum Hochladen, f\u00fchren Sie folgenden Code aus:\")\n",
    "print(\"upload_adapter_to_hub(\")\n",
    "print('    adapter_path=\"./outputs/v1\",')\n",
    "print('    hf_repo_id=\"your-username/your-model-name\",')\n",
    "print('    private_repo=False')\n",
    "print(\")\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9fed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/deploy.py\n",
    "import argparse\n",
    "from huggingface_hub import HfApi, create_repo\n",
    "\n",
    "def main(args):\n",
    "    api = HfApi()\n",
    "    \n",
    "    # 1. Repo-Namen bauen\n",
    "    full_repo_id = f\"{args.username}/{args.repo_name}\"\n",
    "    print(f\"Ziel-Repository: {full_repo_id}\")\n",
    "\n",
    "    # 2. Repository erstellen (falls es noch nicht existiert)\n",
    "    try:\n",
    "        create_repo(full_repo_id, repo_type=\"model\", exist_ok=True)\n",
    "        print(\"Repository gefunden oder erstellt.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Hinweis beim Repo-Erstellen: {e}\")\n",
    "\n",
    "    # 3. Dateien hochladen\n",
    "    print(f\"Lade Ordner '{args.model_dir}' hoch... Bitte warten.\")\n",
    "    \n",
    "    api.upload_folder(\n",
    "        folder_path=args.model_dir,\n",
    "        repo_id=full_repo_id,\n",
    "        repo_type=\"model\",\n",
    "        commit_message=f\"Upload model from production script: {args.repo_name}\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\u2705 Upload erfolgreich!\")\n",
    "    print(f\"Dein Modell ist hier: https://huggingface.co/{full_repo_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--username\", type=str, required=True, help=\"Hugging Face Nutzername\")\n",
    "    parser.add_argument(\"--repo_name\", type=str, required=True, help=\"Name f\u00fcr das neue Modell auf HF\")\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=\"./outputs/final_model\", help=\"Lokaler Pfad zum Modell\")\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca85ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/run_agent.py --adapter \"manuelaschrittwieser/Qwen2.5-SQL-Assistant-Prod\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7862e9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/deploy.py \\\n",
    "    --username \"manuelaschrittwieser\" \\\n",
    "    --repo_name \"Qwen2.5-SQL-Assistant-Prod\" \\\n",
    "    --model_dir \"./outputs/v1/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e4cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q torch transformers peft accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c7ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/setup_db.py\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "def create_dummy_db(db_path=\"data/dummy_database.db\"):\n",
    "    os.makedirs(os.path.dirname(db_path), exist_ok=True)\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Tabelle erstellen\n",
    "    cursor.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS employees (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT,\n",
    "        department TEXT,\n",
    "        salary INTEGER,\n",
    "        hire_date DATE\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # Daten einf\u00fcgen\n",
    "    employees = [\n",
    "        (1, 'Alice Smith', 'Sales', 55000, '2021-01-15'),\n",
    "        (2, 'Bob Jones', 'Engineering', 85000, '2020-03-10'),\n",
    "        (3, 'Charlie Brown', 'Sales', 48000, '2022-06-23'),\n",
    "        (4, 'Diana Prince', 'Engineering', 92000, '2019-11-05'),\n",
    "        (5, 'Evan Wright', 'HR', 45000, '2021-09-30')\n",
    "    ]\n",
    "    \n",
    "    cursor.executemany('INSERT OR IGNORE INTO employees VALUES (?,?,?,?,?)', employees)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(f\"\u2705 Datenbank erstellt: {db_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    create_dummy_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ace0a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logge ein mit Token...\n",
      "Lade Basis-Modell (dies kann kurz dauern)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Adapter...\n",
      "Starte Merge-Vorgang...\n",
      "Lade Tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lade Full-Model hoch zu: manuelaschrittwieser/Qwen2.5-SQL-Assistant-Full ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "034eef0f9cbc4d4fae2d23590b26d6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95592852f658492b873ed7f58104746b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055884bdd49b4b88bd5504e22afad3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...mq7ujzs/model.safetensors:   0%|          | 21.6kB / 3.09GB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004a5b519be345dbb4b1482926d6a565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf981bdbe59042d1bba243e0fd77d6ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0)      : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b7ad799283741709c21f2cdbe216b75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload               : |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07724f832b6146d8b413feca61853be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  ...mpik19yg4d/tokenizer.json:  73%|#######2  | 8.30MB / 11.4MB            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Erfolgreich hochgeladen!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ==========================================\n",
    "# KONFIGURATION\n",
    "# ==========================================\n",
    "# 1. Hugging Face Token ein (mit \"Write\"-Rechten!):\n",
    "HF_TOKEN = \"YOUR_HF_TOKEN_HERE\" \n",
    "\n",
    "# Namen der Modelle\n",
    "BASE_MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "ADAPTER_ID = \"manuelaschrittwieser/Qwen2.5-SQL-Assistant-Prod\"\n",
    "NEW_FULL_MODEL_ID = \"manuelaschrittwieser/Qwen2.5-SQL-Assistant-Full\"\n",
    "\n",
    "# ==========================================\n",
    "# AUTHENTIFIZIERUNG\n",
    "# ==========================================\n",
    "print(f\"Logge ein mit Token...\")\n",
    "login(token=HF_TOKEN)\n",
    "\n",
    "# ==========================================\n",
    "# 1. LADEN (Base + Adapter)\n",
    "# ==========================================\n",
    "print(\"Lade Basis-Modell (dies kann kurz dauern)...\")\n",
    "# Hinweis: Auf CPU ist float32 sicherer als float16, um Abst\u00fcrze zu vermeiden.\n",
    "# Falls RAM-Probleme, wechsele zur\u00fcck auf torch.float16\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_ID,\n",
    "    device_map=\"cpu\",\n",
    "    torch_dtype=torch.float16, \n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(\"Lade Adapter...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model, \n",
    "    ADAPTER_ID, \n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# ==========================================\n",
    "# 2. MERGEN\n",
    "# ==========================================\n",
    "print(\"Starte Merge-Vorgang...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "# ==========================================\n",
    "# 3. SPEICHERN & UPLOAD\n",
    "# ==========================================\n",
    "print(\"Lade Tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "print(f\"Lade Full-Model hoch zu: {NEW_FULL_MODEL_ID} ...\")\n",
    "\n",
    "# Upload des Modells\n",
    "merged_model.push_to_hub(NEW_FULL_MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "# Upload des Tokenizers\n",
    "tokenizer.push_to_hub(NEW_FULL_MODEL_ID, token=HF_TOKEN)\n",
    "\n",
    "print(\"\u2705 Erfolgreich hochgeladen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6fc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile scripts/run_agent.py\n",
    "import sqlite3\n",
    "import torch\n",
    "import argparse\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "class SQLAgent:\n",
    "    def __init__(self, base_model_id, adapter_id, db_path):\n",
    "        self.db_path = db_path\n",
    "        print(\"\ud83e\udd16 Lade das Gehirn des Agenten...\")\n",
    "        \n",
    "        # Modell laden\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_id, \n",
    "            device_map=\"auto\", \n",
    "            torch_dtype=torch.float16\n",
    "        )\n",
    "        self.model = PeftModel.from_pretrained(base_model, adapter_id)\n",
    "        \n",
    "    def generate_sql(self, question, schema_context):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a SQL expert.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{schema_context}\\nQuestion: {question}\"}\n",
    "        ]\n",
    "        prompt = self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=100)\n",
    "            \n",
    "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        # Extrahiere alles nach 'assistant'\n",
    "        if \"assistant\" in full_text:\n",
    "            return full_text.split(\"assistant\")[-1].strip()\n",
    "        return full_text\n",
    "\n",
    "    def execute_sql(self, query):\n",
    "        try:\n",
    "            conn = sqlite3.connect(self.db_path)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            conn.close()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            return f\"Fehler bei SQL-Ausf\u00fchrung: {e}\"\n",
    "\n",
    "    def run(self):\n",
    "        schema = \"CREATE TABLE employees (id INTEGER, name TEXT, department TEXT, salary INTEGER, hire_date DATE)\"\n",
    "        print(\"\\n\u2705 Agent bereit! Tippe 'exit' zum Beenden.\")\n",
    "        \n",
    "        while True:\n",
    "            # Hier wartet Colab auf deine Eingabe\n",
    "            user_input = input(\"\\nDeine Frage an die Datenbank: \")\n",
    "            \n",
    "            if user_input.lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"\ud83d\udc4b Bis bald!\")\n",
    "                break\n",
    "                \n",
    "            # 1. Denken (SQL generieren)\n",
    "            sql = self.generate_sql(user_input, schema)\n",
    "            print(f\"\ud83e\udde0 Gedanke (SQL): {sql}\")\n",
    "            \n",
    "            # 2. Handeln (SQL ausf\u00fchren)\n",
    "            data = self.execute_sql(sql)\n",
    "            \n",
    "            # 3. Antworten\n",
    "            print(f\"\ud83d\udcca Ergebnis aus DB: {data}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--adapter\", type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    agent = SQLAgent(\n",
    "        base_model_id=\"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "        adapter_id=args.adapter,\n",
    "        db_path=\"data/dummy_database.db\"\n",
    "    )\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73dfe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/setup_db.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd75c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir evaluieren das Modell aus dem Ordner 'outputs/v1'\n",
    "!python scripts/evaluate.py \\\n",
    "    --adapter_path \"./outputs/v1\" \\\n",
    "    --num_samples 50"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}